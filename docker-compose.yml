# Ollama is now running locally on the host machine (not in Docker)
# to resolve HTTP 500 errors caused by containerized Ollama issues.

services:
  qdrant:
    image: qdrant/qdrant
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage

  reranker:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.8
    container_name: reranker
    command: --model-id cross-encoder/ms-marco-MiniLM-L-6-v2
    ports:
      - "8080:80"
    volumes:
      - reranker_data:/data

volumes:
  qdrant_data:
    name: qdrant_data
  reranker_data:
    name: reranker_data
